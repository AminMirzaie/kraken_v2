{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# goal of this notebook:\n",
    "### in this notebook for every file we want to find all the minimizers of each kmer and use that to write the kmer and the appropriate taxid of this kmer, in certain place of file.\n",
    "suppous for each minimizer we create folder and we store all the kmer associated to this minimizer in that folder, now for writing new kmer , for example for kmer number 1 in 204669_kmer.fq we find minimizer minimizer(k1) and check the directory of ./minimizer(k1)/ . if the folder not exist , we  make directory minimizer(k1) and put kmer1 and  204669 in this directory. but if the minimizer(k1) exist already, we check if kmer1 exist already in this directory? if yes , we change the taxid of kmer 1 in this directory to LCA(ExistTaxid,204669). if kmer not exist in minimizer(k1) directory we add kmer 1 and 204669 to the directory.\n",
    "\n",
    "### this was abstract review of what we will do in this document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import NCBITaxa\n",
    "import random\n",
    "import glob\n",
    "import time\n",
    "import mmh3\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function give us common ancestor of each two node in tree. If two taxid is equal we return that node and if it was diffrent we use built in function from NCBItaxa package to get taxid of common ancestor of those two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLCA(taxid1,taxid2,tree):\n",
    "    if taxid1 == taxid2:\n",
    "        return taxid1\n",
    "    else:\n",
    "        return int(tree.get_common_ancestor(str(taxid1),str(taxid2)).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the problem of using minimizer as key for kmers in lexical order is that :\n",
    "#### it tend to find low complexity String as minimizer for example AAAAAAA. and we have many low complexity strings in all the genomes. so it result to very large directory /AAAAAAA/ . the reason of this event is  there is lot of kmers in genomes that has AAAAAAA substring. if the size of /AAAAAAA/ become very large it is dificult to load all this data in directory to cache of cpu.\n",
    "for prevent this problem we produce random number and by xoring this random number to each string the lexical order of string will be shuffled. for example in some case AAAAAA is biger than TAAAAA\n",
    "in this cell we produce that random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "n = 31\n",
    "const = int(''.join([\"{}\".format(random.randint(0, 9)) for num in range(0, n)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this cell given string we replace all character with number and we xor const by the reason I explained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNum(st):\n",
    "    conv = {\"A\":1,\"C\":2,\"G\":3,\"T\":4}\n",
    "    temp = 0\n",
    "    l = len(st)\n",
    "    for i in range(len(st)):\n",
    "        temp+=conv[st[i]]*(10**(l-i-1))\n",
    "    return int((temp ^ const%(10**(len(st)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the reason that I dont use jellyfish is that jelly fish change the order of kmers.\n",
    "### but why we need the kmers be ordered?\n",
    "### we can find minimizers in two ways:\n",
    " 1- slide windows across forward and reverse complement of string and find minimum substring among all of those<br>\n",
    " 2- hold last string and its minimizer that we calculate minimizer for that and given new string, we only check the last and start of oldstring and new string(with just two window).  if minimizer was in first of old string with high probablity the minimizer of new string is changed and we use method 1 but if it is not we then check las of new string and compare its minimizer with old minimizer . if new minimizer is smaller than old minimizer we update minimizer\n",
    "\n",
    "the method 2 is not depend on size of string and is much faster for larg n but to use approach 2 the kmers should be ordered.I implemnt these two aproach in next two cells. (we use method 1 when we dont have old string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinimizer(seq):\n",
    "    rev=seq[::-1]\n",
    "    rev=rev.replace(\"A\",\"X\")\n",
    "    rev=rev.replace(\"T\",\"A\")\n",
    "    rev=rev.replace(\"X\",\"T\")\n",
    "    rev=rev.replace(\"C\",\"X\")\n",
    "    rev=rev.replace(\"G\",\"C\")\n",
    "    rev=rev.replace(\"X\",\"G\")\n",
    "    Kmer=len(seq)\n",
    "    M= (Kmer/5) +1\n",
    "    L=len(seq)\n",
    "    min=int(''.join([\"{}\".format(random.randint(9, 9)) for num in range(0, M)]))\n",
    "    for j in range(0, Kmer-M+1):\n",
    "        if j == 0:\n",
    "            sub2=seq[j:j+M]\n",
    "            if getNum(sub2) < min:\n",
    "                min=sub2\n",
    "            sub2=rev[j:j+M]\n",
    "            if getNum(sub2) < min:\n",
    "                min=sub2\n",
    "        else:\n",
    "            sub2=seq[j:j+M]\n",
    "            if getNum(sub2) < getNum(min):\n",
    "                min=sub2\n",
    "            sub2=rev[j:j+M]\n",
    "            if getNum(sub2) < getNum(min):\n",
    "                min=sub2\n",
    "    return min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateMinimizer(seq,oldseq,minimizer):\n",
    "    revseq=seq[::-1]\n",
    "    revseq=revseq.replace(\"A\",\"X\")\n",
    "    revseq=revseq.replace(\"T\",\"A\")\n",
    "    revseq=revseq.replace(\"X\",\"T\")\n",
    "    revseq=revseq.replace(\"C\",\"X\")\n",
    "    revseq=revseq.replace(\"G\",\"C\")\n",
    "    revseq=revseq.replace(\"X\",\"G\")\n",
    "    Kmer=len(seq)\n",
    "    M= (Kmer/5) +1\n",
    "    firstMinimizerOfOld = oldseq[0:M]\n",
    "    revseqOld=firstMinimizerOfOld[::-1]\n",
    "    revseqOld=revseqOld.replace(\"A\",\"X\")\n",
    "    revseqOld=revseqOld.replace(\"T\",\"A\")\n",
    "    revseqOld=revseqOld.replace(\"X\",\"T\")\n",
    "    revseqOld=revseqOld.replace(\"C\",\"X\")\n",
    "    revseqOld=revseqOld.replace(\"G\",\"C\")\n",
    "    revseqOld=revseqOld.replace(\"X\",\"G\")\n",
    "    L=len(seq)\n",
    "    end_f = seq[L-M:L]\n",
    "    start_f = seq[0:M]\n",
    "    end_r = revseq[L-M:L]\n",
    "    start_r = revseq[0:M]\n",
    "    minNum = getNum(minimizer)    \n",
    "    if (getNum(revseqOld) == minNum) or (getNum(firstMinimizerOfOld) == minNum):\n",
    "        if(getMinimizer(seq) == minNum):\n",
    "            return False,minimizer\n",
    "        else:\n",
    "            return True,getMinimizer(seq)\n",
    "    flag = False\n",
    "    if (getNum(end_f) < minNum):\n",
    "        minimizer = end_f\n",
    "    if(getNum(start_r) < minNum):\n",
    "        flag = True\n",
    "        minimizer = start_r\n",
    "    return flag,minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by making small each kmer and each minimizer our performance become better and better.\n",
    "instead of writing each kmer and minimizer in file I first hash them and then use their hash as key and value in file. on average this work take half of size than last approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHashedKmer(kmer):\n",
    "    return mmh3.hash(kmer,seed=1,signed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHashedMinimizer(minimizer):\n",
    "    return mmh3.hash(minimizer,seed=2,signed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the main part:\n",
    "#### now we have all the functions that we need\n",
    "as I explain first of this note book we should consider one path for each minimizer and store all the kmer,taixd into that path. I use a dictionary that has key and value. the key of this dictionary is my minimizer and the value of this dictionary is another dictionary that contain all the kmers related to that minimizer as key and taxids as value.\n",
    "#### the goal of this part is to construct this huge dictionary in Ram\n",
    "first i Itterate on the kmer files and for each kmer I find Hashed minimizer and Hashed kmer and I will check if the index[minimizer] not exist I make it and sore the {Hkmer:taxid} on it.\n",
    "if it exist I go to the directory and check if index[Hmin][Hkmer] exist or not? if it exist I calculate Lca of new taxid and old taxid and if it not exist i just write it to that directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file  1198114_kmer.fqprocessed in this time:\n",
      "--- 250.392916203 seconds ---\n",
      "file  1855912_kmer.fqprocessed in this time:\n",
      "--- 337.673550129 seconds ---\n",
      "file  1592106_kmer.fqprocessed in this time:\n",
      "--- 290.066696167 seconds ---\n",
      "file  981222_kmer.fqprocessed in this time:\n",
      "--- 164.681408882 seconds ---\n",
      "file  401053_kmer.fqprocessed in this time:\n",
      "--- 232.843212128 seconds ---\n",
      "file  204669_kmer.fqprocessed in this time:\n",
      "--- 255.945392847 seconds ---\n",
      "file  234267_kmer.fqprocessed in this time:\n",
      "--- 448.328229189 seconds ---\n",
      "file  240015_kmer.fqprocessed in this time:\n",
      "--- 188.423882961 seconds ---\n",
      "file  926566_kmer.fqprocessed in this time:\n",
      "--- 241.118476152 seconds ---\n",
      "file  682795_kmer.fqprocessed in this time:\n",
      "--- 287.801056147 seconds ---\n",
      "file  2211140_kmer.fqprocessed in this time:\n",
      "--- 349.414181948 seconds ---\n"
     ]
    }
   ],
   "source": [
    "fileNames = (glob.glob(\"../genomes/*.fq\"))\n",
    "ncbi = NCBITaxa()\n",
    "ParentNode=\"Acidobacteria\" #inja ro bayad karbar bde az kojaye derakht b paeen mikhad\n",
    "ParentTaxid = ncbi.get_name_translator([ParentNode])[ParentNode][0]\n",
    "tree = ncbi.get_descendant_taxa(ParentNode, return_tree=True)\n",
    "index = {}\n",
    "for PathFilename in fileNames:\n",
    "    taxid = int(PathFilename.split(\"_\")[0].split(\"/\")[2])\n",
    "    start_time = time.time()\n",
    "    with open(PathFilename) as f:\n",
    "        count = 0\n",
    "        oldkmer = \"\"\n",
    "        oldMinimizer = \"\"\n",
    "        for kmer in f:\n",
    "            kmer = kmer.strip()\n",
    "            if count == 0:\n",
    "                minimizer = getMinimizer(kmer)\n",
    "                oldkmer = kmer\n",
    "                oldMinimizer = minimizer\n",
    "            else:\n",
    "                flag,minimizer = updateMinimizer(kmer,oldkmer,oldMinimizer)\n",
    "                oldkmer = kmer\n",
    "                oldMinimizer = minimizer\n",
    "            Hmin = getHashedMinimizer(minimizer)\n",
    "            Hkmer =getHashedKmer(kmer)\n",
    "            if Hmin in index:\n",
    "                if Hkmer in index[Hmin]:\n",
    "                    index[Hmin][Hkmer] = getLCA(index[Hmin][Hkmer],taxid,tree)\n",
    "                else:\n",
    "                    index[Hmin][Hkmer] = taxid\n",
    "            else:\n",
    "                index[Hmin] = {Hkmer:taxid}\n",
    "            count +=1\n",
    "    print \"file  \" + str(taxid)+ \"_kmer.fq\" + \"processed in this time:\"\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can not hold this dictionary in RAM beacause we should represent this file beside our package and user just use this file to classify read of samples.\n",
    " ### but there is a problem if user read all the index file, because it cause all the index file come into Ram again.\n",
    "### we need file structure that has this ability to load certain part of file I used HDF5 file format.\n",
    " in this file format we can have two things: 1- group 2- data set. data sets is just like numpy array in python and group is just path in linux or windows. for example I stored numpy array in group \"aaa\". I can access that array by this adress: \"/aaa/\". the benefit of using this file format is that we dont need to Load all the the index file. in the next notebook we want to find taxids related to certain read. so we find each kmer of read and find minimizer and then load /minimizer/ data set in index file not all the file. for next kmer with high probablity there is no need to load another data set beacause it may hava same minimizer as last kmer.\n",
    "in this cell we convert all the inner dictionary to numpy array with two row. row 1 refers to Hashed kmer and row 2 refers to taxids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary in index file finished in time:\n",
      "--- 52.171806097 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with h5py.File(\"../resources/index.hdf5\", \"w\") as f:    \n",
    "    for key in index:\n",
    "        minim = key\n",
    "        arr = np.row_stack((index[key].keys(),index[key].values()))\n",
    "        dset = f.create_dataset(str(minim)+'/list',(2,arr.shape[1]), dtype='i')\n",
    "        dset[...] = arr\n",
    "print \"dictionary in index file finished in time:\"\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "index = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the result of this note book is index file located in resources path of project\n",
    "### in the next notebook named readClassifier we use Fastq file that contain noisy reads from these 11 files with specific abundance of each genomes and will write code to predict the abundance from Fastq file and index. then we compare our result to the real abundancy of each genomes\n",
    "please open readClassifier notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
